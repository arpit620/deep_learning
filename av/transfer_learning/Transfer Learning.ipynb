{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning & The art of using Pre-trained Models in Deep Learning\n",
    "[Article](https://analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above mentioned article explains the theoretical aspect of Transfer Learning really well but comes a little short on when it comes to implementation of the same. \n",
    "There were lot of issues which I faced while trying to execute the code directly from the website, like:\n",
    "- Finding the dataset used ( The dataset link provided in article isn't the one used in implementation )\n",
    "- MNIST images are of dimensions 28x28 and when scaled to 224x224 takes up whole memory ( Tried on 32GB RAM server) and crashes it.\n",
    "- In the second part of implementation \"Freeze the weigts of first few layers\", the CNN architecture which is created is not correct as it adds the final 10 class neuron layer on top of previous 1000 class neuron layer while we are suppose to replace the last layer instead of adding on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Instead of loading the whole dataset in memory at one go, there is another better and optimized way of doing it, which is using \"flow_from_directory\" method of Keras.\n",
    "If time permits I would be writing another notebook covering that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This command specify which GPU to use in case multiple GPU's are available.\n",
    "\n",
    "This notebook was run on GTX 980 Ti GPU, so you might observe different run times on your end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Retrain the output dense layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpit/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "\n",
    "from keras.models import Sequential\n",
    "from scipy.misc import imread\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Download\n",
    "\n",
    "Data for this notebook can be downloaded from \"Identify the Digits\" hackathon from Analytics Vidhya website using the following link:<br>\n",
    "https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/ \n",
    "\n",
    "Extract the compressed file to a specific location and rename the csv files to train.csv and test.csv respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Pass on the csv files path to first two variables and provide the image folder path to other two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw=pd.read_csv(\"/home/arpit/notebooks/data/av/mnist/train.csv\")\n",
    "test_raw=pd.read_csv(\"/home/arpit/notebooks/data/av/mnist/test.csv\")\n",
    "train_path=\"/home/arpit/notebooks/data/av/mnist/Images/train/\"\n",
    "test_path=\"/home/arpit/notebooks/data/av/mnist/Images/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the data sampling by picking only 10% of the data as the main focus for this tutorial is to get up and running on Transfer Learning rather than focusing on accurcy of the data.\n",
    "\n",
    "You can increase or decrease the sampling percent as per your convenience. \n",
    "\n",
    "Sampling here is important as if you try to load the provided training dataset which has 49,000 images in the way they have implemented you won't be able to load it fully as it takes everything in RAM and even after having 32GB RAM my server crashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of images in raw training file\n",
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.1\n",
    "train = train_raw.sample(frac=data_sampling_rate).reset_index(drop=True)\n",
    "test = test_raw.sample(frac=data_sampling_rate).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of images after sampling\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imresize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the dataset\n",
    "- Initialize empty array\n",
    "- Read the sampled csv file line by line, find the image, upscale it to size 224x224 from the default of 28x28\n",
    "- Conver the image to array\n",
    "- Append it the list\n",
    "- Convert it to Numpy array\n",
    "- Do some Preprocessing like normalizing pixels, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the train dataset\n",
    "train_img=[]\n",
    "for i in range(len(train)):\n",
    "    temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224))\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    train_img.append(temp_img)\n",
    "\n",
    "#converting train images to array and applying mean subtraction processing\n",
    "train_img=np.array(train_img) \n",
    "train_img=preprocess_input(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the same procedure with the test dataset\n",
    "test_img=[]\n",
    "for i in range(len(test)):\n",
    "    temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224))\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    test_img.append(temp_img)\n",
    "\n",
    "#converting test images to array and applying mean subtraction processing\n",
    "test_img=np.array(test_img) \n",
    "test_img=preprocess_input(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the VGG16 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the architecture of VGG16 model:\n",
    "- Default input image size the network was trained on was 224x224\n",
    "- The last MaxPooling layer outputs 7x7x512 which when flattened gives 25088 neuros ( used later when reshaping the image features trained on Convolution layers)\n",
    "\n",
    "To save further on memory and trying out only the Part1 of transfer learning, you can resize the image further to smaller dimensions upto 48x48 but that will change the output size from 7x7x512 to some other dimension, which you need to change manually later in the code. For further details on input image size check the __input_shape__ argument on the following link:<br>\n",
    "https://keras.io/applications/#vgg16\n",
    "\n",
    "Keras implementation of VGG16:<br>\n",
    "https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16.png](../../images/VGG16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__include_top = False__ loads only the Convolutional layers of VGG Model and removes the Dense layer part\n",
    "- It removes the last 4 layers from architecture ( 3 blue fully connected layer + 1 brown softmax classification layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading VGG16 model weights\n",
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two commands of Feature extraction on Training and Testing dataset can take some time to process on CPU only machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 5.98 s, total: 1min 13s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "features_train=model.predict(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 2.62 s, total: 31.7 s\n",
      "Wall time: 8.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extracting features from the test dataset using the VGG16 pre-trained model\n",
    "features_test=model.predict(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first number represent the number of images which we are going to process which is 4900 in this case\n",
    "- Last 3 numbers represents the dimenstions of final MaxPooling layer i.e. 7x7x512 = 25088 neurons when flattened.\n",
    "- In case you have changed the target size of images from 224x224 while loading it up to anything above 48x48, multiplying these last 3 numbers will give the other parameter value in the reshape funcation used below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900, 7, 7, 512)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4900"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made the first parametere of reshape command to dynamic so it can be picked automatically even if you decide to take a different sample size.\n",
    "\n",
    "The __.reshape__ method takes two arguments:\n",
    "- First argument: Number of rows/ images\n",
    "- Seocond argument: Number of neurons we will get on flattening the output of final MaxPooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening the layers to conform to MLP input\n",
    "# train_x=features_train.reshape(49000,25088)\n",
    "train_x=features_train.reshape(features_train.shape[0],25088)\n",
    "\n",
    "# converting target variable to array\n",
    "train_y=np.asarray(train['label'])\n",
    "\n",
    "# performing one-hot encoding for the target variable\n",
    "train_y=pd.get_dummies(train_y)\n",
    "train_y=np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(train_x,train_y,\\\n",
    "                                        test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further change the __input_dim__ parameter in case you lowered the target_size dimension originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a mlp model\n",
    "from keras.layers import Dense, Activation\n",
    "model=Sequential()\n",
    "\n",
    "# Change input_dim if required\n",
    "model.add(Dense(1000, input_dim=25088, activation='relu',kernel_initializer='uniform'))\n",
    "keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(500,input_dim=1000,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(150,input_dim=500,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3430 samples, validate on 1470 samples\n",
      "Epoch 1/10\n",
      "3430/3430 [==============================] - 1s 290us/step - loss: 1.2035 - acc: 0.7942 - val_loss: 0.4961 - val_acc: 0.9435\n",
      "Epoch 2/10\n",
      "3430/3430 [==============================] - 1s 197us/step - loss: 0.2829 - acc: 0.9612 - val_loss: 0.1819 - val_acc: 0.9667\n",
      "Epoch 3/10\n",
      "3430/3430 [==============================] - 1s 197us/step - loss: 0.1127 - acc: 0.9854 - val_loss: 0.1211 - val_acc: 0.9755\n",
      "Epoch 4/10\n",
      "3430/3430 [==============================] - 1s 196us/step - loss: 0.0637 - acc: 0.9921 - val_loss: 0.0974 - val_acc: 0.9755\n",
      "Epoch 5/10\n",
      "3430/3430 [==============================] - 1s 198us/step - loss: 0.0366 - acc: 0.9959 - val_loss: 0.0794 - val_acc: 0.9769\n",
      "Epoch 6/10\n",
      "3430/3430 [==============================] - 1s 197us/step - loss: 0.0197 - acc: 0.9994 - val_loss: 0.0722 - val_acc: 0.9810\n",
      "Epoch 7/10\n",
      "3430/3430 [==============================] - 1s 198us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.0676 - val_acc: 0.9810\n",
      "Epoch 8/10\n",
      "3430/3430 [==============================] - 1s 197us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0659 - val_acc: 0.9816\n",
      "Epoch 9/10\n",
      "3430/3430 [==============================] - 1s 198us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0664 - val_acc: 0.9810\n",
      "Epoch 10/10\n",
      "3430/3430 [==============================] - 1s 198us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0665 - val_acc: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4605052358>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=10, batch_size=128,validation_data=(X_valid,Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "You can see the importance of Transfer learning as despite having only 10% of data sampling we were able to achieve 100% accuracy on training data and 98.2% accuracy on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Freeze the weights of first few layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 2, we will be using the same architecture as that of original VGG16, where we will be freezing the first 15 layers ( till 4th red color Max Pooling layer) and will be retraining the weights for the last 8 layers.\n",
    "\n",
    "We also need to replace the last layer of 1000 classes with another layer of 10 classes for Digit classification.\n",
    "\n",
    "Original article on Analytics Vidhya ended up adding another layer on top of 1000 neuron classes while we actually have to replace it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16.png](../../images/VGG16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part till loading up the train and test data is similar to that of the above. For detailed explanation check Part 1 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Visibility\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpit/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from keras.models import Sequential\n",
    "from scipy.misc import imread\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw=pd.read_csv(\"/home/arpit/notebooks/data/av/mnist/train.csv\")\n",
    "test_raw=pd.read_csv(\"/home/arpit/notebooks/data/av/mnist/test.csv\")\n",
    "train_path=\"/home/arpit/notebooks/data/av/mnist/Images/train/\"\n",
    "test_path=\"/home/arpit/notebooks/data/av/mnist/Images/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.1\n",
    "train = train_raw.sample(frac=data_sampling_rate).reset_index(drop=True)\n",
    "test = test_raw.sample(frac=data_sampling_rate).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4900, 2)\n",
      "(2100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imresize\n",
    "\n",
    "train_img=[]\n",
    "for i in range(len(train)):\n",
    "    temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224))\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    train_img.append(temp_img)\n",
    "\n",
    "train_img=np.array(train_img) \n",
    "train_img=preprocess_input(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img=[]\n",
    "for i in range(len(test)):\n",
    "    temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224))\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    test_img.append(temp_img)\n",
    "\n",
    "test_img=np.array(test_img) \n",
    "test_img=preprocess_input(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding of the below function, read the following page from Keras documentation:<br>\n",
    "https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "def vgg16_model(channel=1, num_classes=None):\n",
    "\n",
    "    # Loads the complete VGG16 model, including the top dense layer\n",
    "    model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "    # Removes the last layer of 1000 classes\n",
    "    model.layers.pop()\n",
    "\n",
    "    # Makes the model output point to output of second last layer i.e. the one with 4096 neurons\n",
    "    model.outputs = [model.layers[-1].output]\n",
    "    \n",
    "    # Removes the connection between neurons of second last layer and orignal last layer of 1000 classes\n",
    "    model.layers[-1].outbound_nodes = []\n",
    "    \n",
    "    # Original Article\n",
    "    # \"model.output\" still has details regarding the orignal 1000 classes\n",
    "    # So that can't be used as it ends up adding the 10 class neuron on top of 1000 class neuron\n",
    "    # x=Dense(num_classes, activation='softmax')(model.output)\n",
    "    \n",
    "    # Modified\n",
    "    # This adds the newly created 10 class layers on top of the output from second last layer\n",
    "    x=Dense(num_classes, activation='softmax')(model.outputs[0])\n",
    "        \n",
    "    # Defining the model architecture\n",
    "    model=Model(model.input,x)\n",
    "\n",
    "    #To set the first 15 layers to non-trainable (weights will not be updated)\n",
    "    # Originally set to 8, updated it to 15 to reduce the training time.\n",
    "    for layer in model.layers[:15]:\n",
    "       layer.trainable = False\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "train_y=np.asarray(train['label'])\n",
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(train_y)\n",
    "train_y=to_categorical(train_y)\n",
    "train_y=np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "channel = 3\n",
    "num_classes = 10 \n",
    "batch_size = 16 \n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the below model summary that in the last, we have only only 10 layer and we have replaced the 1000 class layer successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 126,666,250\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load our model\n",
    "model = vgg16_model( channel, num_classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- In above summary, in the last part, it shows the total Trainable parameters are ~127 Million, which is actually a lot to train, but as we are loading the pre-trained model, the initialized weights are already relevant and so it converges faster as compared to training it from scratch where we initilize the weights randomly.\n",
    "\n",
    "***\n",
    "- Below times are on GPU. Training the same on CPU can take a lot more time\n",
    "- Maybe you can try it out with lesser epoch's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7fb4bca36c50> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a68b2cf8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a68f4e10> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fb4a6646a58> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a666b898> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a6605ef0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fb4a661cfd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a65c9128> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a65c9400> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a65dbf98> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fb4a6581780> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a65ac198> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a65ac470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a6558a90> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fb4a6569860> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a6515208> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a65154e0> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fb4a64c0b00> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fb4a64cf860> True\n",
      "<keras.layers.core.Flatten object at 0x7fb4a647c2b0> True\n",
      "<keras.layers.core.Dense object at 0x7fb4a647c588> True\n",
      "<keras.layers.core.Dense object at 0x7fb4a64a7ac8> True\n",
      "<keras.layers.core.Dense object at 0x7fb4a6453b38> True\n"
     ]
    }
   ],
   "source": [
    "# Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 3920 samples, validate on 980 samples\n",
      "Epoch 1/10\n",
      "3920/3920 [==============================] - 35s 9ms/step - loss: 0.8696 - acc: 0.7253 - val_loss: 0.2410 - val_acc: 0.9235\n",
      "Epoch 2/10\n",
      "3920/3920 [==============================] - 33s 8ms/step - loss: 0.1447 - acc: 0.9548 - val_loss: 0.1374 - val_acc: 0.9663\n",
      "Epoch 3/10\n",
      "3920/3920 [==============================] - 33s 8ms/step - loss: 0.0816 - acc: 0.9747 - val_loss: 0.1290 - val_acc: 0.9602\n",
      "Epoch 4/10\n",
      "3920/3920 [==============================] - 33s 8ms/step - loss: 0.0579 - acc: 0.9842 - val_loss: 0.1295 - val_acc: 0.9684\n",
      "Epoch 5/10\n",
      "3920/3920 [==============================] - 33s 8ms/step - loss: 0.0349 - acc: 0.9890 - val_loss: 0.1385 - val_acc: 0.9653\n",
      "Epoch 6/10\n",
      "3920/3920 [==============================] - 33s 8ms/step - loss: 0.0283 - acc: 0.9906 - val_loss: 0.1423 - val_acc: 0.9694\n",
      "Epoch 7/10\n",
      "3920/3920 [==============================] - 33s 8ms/step - loss: 0.0148 - acc: 0.9967 - val_loss: 0.1234 - val_acc: 0.9684\n",
      "Epoch 8/10\n",
      "3920/3920 [==============================] - 33s 8ms/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.0918 - val_acc: 0.9806\n",
      "Epoch 9/10\n",
      "3920/3920 [==============================] - 33s 8ms/step - loss: 4.9499e-04 - acc: 1.0000 - val_loss: 0.0896 - val_acc: 0.9837\n",
      "Epoch 10/10\n",
      "3920/3920 [==============================] - 33s 8ms/step - loss: 1.2720e-04 - acc: 1.0000 - val_loss: 0.0917 - val_acc: 0.9837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4a62255f8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Fine-tuning\n",
    "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running our model just for 10 epochs, we are able to achieve 100% accuracy on trainig data and 98.3% accuracy on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980/980 [==============================] - 5s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions_valid = model.predict(X_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss score\n",
    "score = log_loss(Y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09168487530338551"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
